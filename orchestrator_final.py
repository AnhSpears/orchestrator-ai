"""
ORCHESTRATOR FINAL - Phi√™n b·∫£n ho√†n ch·ªânh, ƒë√£ fix t·∫•t c·∫£ l·ªói
"""
import sys
import os
import logging
import re
from datetime import datetime

# Th√™m ƒë∆∞·ªùng d·∫´n
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def setup_logging():
    """Thi·∫øt l·∫≠p logging"""
    log_dir = "logs"
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    
    log_file = os.path.join(log_dir, f"orchestrator_final_{datetime.now().strftime('%Y%m%d')}.log")
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    return logging.getLogger(__name__)

def display_final_banner():
    """Hi·ªÉn th·ªã banner cu·ªëi c√πng"""
    banner = """
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë               ORCHESTRATOR AI - FINAL                    ‚ïë
‚ïë                Phi√™n b·∫£n 3.0 - Ho√†n Ch·ªânh                ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

T√çNH NƒÇNG N·ªîI B·∫¨T:
‚úÖ Deepseek-coder: D√πng prompt ti·∫øng Anh cho code
‚úÖ Qwen2.5:14b: Chat ti·∫øng Vi·ªát xu·∫•t s·∫Øc  
‚úÖ Kh√¥ng l·ªói _select_model
‚úÖ Response KH√îNG b·ªã l·∫∑p
‚úÖ X·ª≠ l√Ω command th√¥ng minh

üìã L·ªÜNH ƒê·∫∂C BI·ªÜT:
‚Ä¢ 'tho√°t' / 'exit' - D·ª´ng h·ªá th·ªëng
‚Ä¢ 'model' / 'm√¥ h√¨nh' - Xem th√¥ng tin model
‚Ä¢ 'ch·∫ø ƒë·ªô' / 'mode' - Tr·∫°ng th√°i h·ªá th·ªëng
‚Ä¢ 'help' / 'tr·ª£ gi√∫p' - H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng
‚Ä¢ 'test' / 'ki·ªÉm tra' - Ki·ªÉm tra h·ªá th·ªëng

üí° M·∫∏O D√ôNG:
‚Ä¢ Code: H·ªèi b·∫±ng ti·∫øng Anh ho·∫∑c ti·∫øng Vi·ªát ƒë∆°n gi·∫£n
‚Ä¢ Chat: Ti·∫øng Vi·ªát t·ª± nhi√™n
‚Ä¢ Research: Ti·∫øng Vi·ªát ho·∫∑c Anh ƒë·ªÅu ƒë∆∞·ª£c
"""
    print(banner)

def clean_response_advanced(response: str) -> str:
    """
    L√†m s·∫°ch response c·ª±c m·∫°nh - lo·∫°i b·ªè ho√†n to√†n l·∫∑p
    """
    if not response:
        return ""
    
    # 1. Chia th√†nh ƒëo·∫°n
    paragraphs = response.strip().split('\n\n')
    
    # 2. Lo·∫°i b·ªè ƒëo·∫°n tr√πng l·∫∑p
    unique_paragraphs = []
    seen_content = set()
    
    for para in paragraphs:
        para_clean = para.strip()
        if not para_clean:
            continue
        
        # T·∫°o "signature" c·ªßa ƒëo·∫°n (l·∫•y 100 k√Ω t·ª± ƒë·∫ßu, b·ªè kho·∫£ng tr·∫Øng)
        if len(para_clean) > 100:
            sig = para_clean[:100].lower().replace(' ', '')
        else:
            sig = para_clean.lower().replace(' ', '')
        
        # N·∫øu ch∆∞a th·∫•y signature n√†y
        if sig not in seen_content:
            seen_content.add(sig)
            unique_paragraphs.append(para_clean)
    
    # 3. Gh√©p l·∫°i
    result = '\n\n'.join(unique_paragraphs)
    
    # 4. Lo·∫°i b·ªè d√≤ng tr√πng trong c√πng ƒëo·∫°n
    lines = result.split('\n')
    final_lines = []
    prev_line = ""
    
    for line in lines:
        line_stripped = line.strip()
        if line_stripped and line_stripped != prev_line:
            final_lines.append(line_stripped)
            prev_line = line_stripped
    
    return '\n'.join(final_lines)

def test_system():
    """Ki·ªÉm tra nhanh h·ªá th·ªëng"""
    print("\nüß™ KI·ªÇM TRA NHANH H·ªÜ TH·ªêNG...")
    
    tests = [
        ("K·∫øt n·ªëi Ollama", lambda: check_ollama()),
        ("Import modules", lambda: check_imports()),
        ("Config files", lambda: check_configs())
    ]
    
    for test_name, test_func in tests:
        try:
            if test_func():
                print(f"  ‚úÖ {test_name}")
            else:
                print(f"  ‚ùå {test_name}")
        except Exception as e:
            print(f"  ‚ùå {test_name}: {str(e)[:50]}")

def check_ollama():
    """Ki·ªÉm tra Ollama"""
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        return response.status_code == 200
    except:
        return False

def check_imports():
    """Ki·ªÉm tra import"""
    try:
        from core_ai.brain import Brain
        from chat_module.chat_router import ChatRouter
        return True
    except Exception as e:
        print(f"    L·ªói import: {e}")
        return False

def check_configs():
    """Ki·ªÉm tra config"""
    config_files = ['config/permissions.yaml', 'config/llm_profiles.yaml']
    for file in config_files:
        if not os.path.exists(file):
            return False
    return True

def main():
    """H√†m ch√≠nh phi√™n b·∫£n FINAL"""
    logger = setup_logging()
    
    try:
        display_final_banner()
        test_system()
        
        # Import
        from chat_module.chat_router import ChatRouter
        
        # Kh·ªüi t·∫°o
        logger.info("üöÄ Kh·ªüi ƒë·ªông ORCHESTRATOR FINAL...")
        router = ChatRouter()
        
        # Hi·ªÉn th·ªã th√¥ng tin h·ªá th·ªëng
        print(f"\n{'='*60}")
        print("üìä TH√îNG TIN H·ªÜ TH·ªêNG")
        print(f"{'='*60}")
        print(f"‚Ä¢ Model c√≥ s·∫µn: {len(router.brain.llm_dispatcher.available_models)} model")
        print(f"‚Ä¢ Coding model: {router.brain.llm_dispatcher.model_priority['coding']}")
        print(f"‚Ä¢ Chat model: {router.brain.llm_dispatcher.model_priority['chat']}")
        print(f"‚Ä¢ Ch·∫ø ƒë·ªô: {'DEMO' if router.brain.llm_dispatcher.use_mock else 'FULL'}")
        print(f"{'='*60}")
        
        print("\nüí¨ CHAT ƒê√É S·∫¥N S√ÄNG (g√µ 'tho√°t' ƒë·ªÉ d·ª´ng)")
        
        # Bi·∫øn ƒë·ªÉ ki·ªÉm tra l·∫∑p
        last_response = ""
        repeat_count = 0
        
        while True:
            try:
                # Nh·∫≠p input
                user_input = input("\nüë§ B·∫°n: ").strip()
                
                # X·ª≠ l√Ω empty input
                if not user_input:
                    continue
                
                # Command ƒë·∫∑c bi·ªát
                if user_input.lower() in ['tho√°t', 'exit', 'quit']:
                    print("\nüëã T·∫°m bi·ªát! H·∫πn g·∫∑p l·∫°i.")
                    break
                
                if user_input.lower() in ['model', 'models', 'm√¥ h√¨nh']:
                    print(f"\nü§ñ TH√îNG TIN MODEL:")
                    models = router.brain.llm_dispatcher.available_models
                    print(f"‚Ä¢ T·ªïng s·ªë: {len(models)} model")
                    print(f"‚Ä¢ Coding: {router.brain.llm_dispatcher.model_priority['coding']}")
                    print(f"‚Ä¢ Chat: {router.brain.llm_dispatcher.model_priority['chat']}")
                    print(f"‚Ä¢ Research: {router.brain.llm_dispatcher.model_priority['research']}")
                    print(f"\nüí° G·ª£i √Ω:")
                    print(f"  - Code: D√πng {router.brain.llm_dispatcher.model_priority['coding']} (ti·∫øng Anh)")
                    print(f"  - Chat: D√πng {router.brain.llm_dispatcher.model_priority['chat']} (ti·∫øng Vi·ªát)")
                    continue
                
                if user_input.lower() in ['ch·∫ø ƒë·ªô', 'mode', 'status']:
                    mode = "DEMO" if router.brain.llm_dispatcher.use_mock else "FULL"
                    print(f"\n‚öôÔ∏è TR·∫†NG TH√ÅI H·ªÜ TH·ªêNG:")
                    print(f"‚Ä¢ Ch·∫ø ƒë·ªô: {mode}")
                    print(f"‚Ä¢ Model hi·ªán t·∫°i: {router.brain.llm_dispatcher.model_priority}")
                    print(f"‚Ä¢ K·∫øt n·ªëi Ollama: {'‚ùå Ch∆∞a k·∫øt n·ªëi' if router.brain.llm_dispatcher.use_mock else '‚úÖ ƒê√£ k·∫øt n·ªëi'}")
                    continue
                
                if user_input.lower() in ['help', 'tr·ª£ gi√∫p', 'h∆∞·ªõng d·∫´n']:
                    print(f"\nüìñ H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG ORCHESTRATOR:")
                    print("1. Chat th√¥ng th∆∞·ªùng: Ti·∫øng Vi·ªát/Anh t·ª± nhi√™n")
                    print("2. Vi·∫øt code: D√πng ti·∫øng Anh ho·∫∑c ti·∫øng Vi·ªát ƒë∆°n gi·∫£n")
                    print("   V√≠ d·ª•: 'Write Python function to read CSV'")
                    print("3. Nghi√™n c·ª©u: H·ªèi b·∫±ng ti·∫øng Vi·ªát ho·∫∑c Anh")
                    print("4. L·ªánh: 'model', 'ch·∫ø ƒë·ªô', 'tho√°t', 'help'")
                    print("\n‚ö†Ô∏è L∆ØU √ù: deepseek-coder ch·ªâ hi·ªÉu ti·∫øng Anh cho code")
                    continue
                
                # X·ª≠ l√Ω chat
                print("ü§ñ ƒêang x·ª≠ l√Ω...", end="", flush=True)
                
                # G·ªçi router
                result = router.route(user_input)
                
                # X√≥a d√≤ng "ƒëang x·ª≠ l√Ω"
                print("\r" + " " * 50 + "\r", end="")
                
                if result.get('status') == 'success':
                    llm_result = result.get('result', {})
                    response_text = llm_result.get('response', '')
                    model = llm_result.get('model', 'unknown')
                    mode = llm_result.get('mode', 'real')
                    
                    # L√†m s·∫°ch response c·ª±c m·∫°nh
                    cleaned_response = clean_response_advanced(response_text)
                    
                    # Ki·ªÉm tra l·∫∑p v·ªõi response tr∆∞·ªõc ƒë√≥
                    if cleaned_response == last_response:
                        repeat_count += 1
                        if repeat_count >= 2:
                            print("‚ö†Ô∏è  Ph√°t hi·ªán response l·∫∑p, b·ªè qua...")
                            continue
                    else:
                        repeat_count = 0
                        last_response = cleaned_response
                    
                    # Hi·ªÉn th·ªã
                    if 'coder' in model.lower():
                        print(f"ü§ñ [{model} - Coding Assistant]:")
                    elif mode == 'mock':
                        print(f"ü§ñ [Demo Mode]:")
                    else:
                        print(f"ü§ñ [{model}]:")
                    
                    print(f"{cleaned_response}\n")
                    
                    # Log
                    logger.info(f"Input: {user_input[:50]}... | Model: {model} | Len: {len(response_text)}")
                    
                else:
                    print(f"‚ùå L·ªói: {result.get('error', 'Kh√¥ng x√°c ƒë·ªãnh')}\n")
                    logger.error(f"Error: {result.get('error')}")
                
            except KeyboardInterrupt:
                print("\n\nüõë ƒê√£ d·ª´ng b·ªüi ng∆∞·ªùi d√πng")
                break
            except Exception as e:
                print(f"\n‚ö†Ô∏è  L·ªói h·ªá th·ªëng: {str(e)[:100]}")
                logger.error(f"L·ªói trong chat: {e}")
    
    except Exception as e:
        logger.critical(f"L·ªói kh·ªüi ƒë·ªông h·ªá th·ªëng: {e}")
        print(f"‚ùå L·ªói nghi√™m tr·ªçng: {e}")
        return 1
    
    logger.info("H·ªá th·ªëng ƒë√£ d·ª´ng")
    return 0

if __name__ == "__main__":
    sys.exit(main())