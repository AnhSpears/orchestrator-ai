"""
LLM DISPATCHER Tá»I Æ¯U - Sá»­ dá»¥ng model phÃ¹ há»£p cho tá»«ng task
"""
import yaml
import logging
import time
import requests
from typing import Dict, Any
import random

class LLMDispatcher:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.ollama_base = "http://localhost:11434"
        self.use_mock = False
        
        # Táº£i cáº¥u hÃ¬nh
        self.llm_profiles = self._load_llm_profiles()
        
        # Khá»Ÿi táº¡o model
        self.available_models = []
        self.model_priority = {}
        self._initialize_dispatcher()
        
    def _load_llm_profiles(self):
        """Táº£i cáº¥u hÃ¬nh LLM tá»« file"""
        try:
            with open('config/llm_profiles.yaml', 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            self.logger.warning(f"KhÃ´ng thá»ƒ táº£i llm_profiles: {e}")
            return {
                "profiles": {
                    "chat": {"models": ["llama3:8b"]},
                    "coding": {"models": ["deepseek-coder:6.7b"]},
                    "research": {"models": ["llama3:8b"]},
                    "web_search": {"models": ["llama3:8b"]}
                }
            }
    
    def _initialize_dispatcher(self):
        """Khá»Ÿi táº¡o dispatcher vá»›i cÃ¡c model cÃ³ sáºµn"""
        try:
            self._detect_available_models()
            if not self.available_models:
                self.logger.warning("KhÃ´ng cÃ³ model nÃ o, chuyá»ƒn sang mock mode")
                self.use_mock = True
            else:
                self._select_optimal_model()
                self.logger.info(f"Dispatcher khá»Ÿi táº¡o thÃ nh cÃ´ng. Model cÃ³ sáºµn: {self.available_models}")
        except Exception as e:
            self.logger.error(f"Lá»—i khá»Ÿi táº¡o dispatcher: {e}")
            self.use_mock = True
    
    def _detect_available_models(self):
        """PhÃ¡t hiá»‡n model nÃ o thá»±c sá»± cÃ³ sáºµn vÃ  hoáº¡t Ä‘á»™ng"""
        self.logger.info("ğŸ” Äang phÃ¡t hiá»‡n model cÃ³ sáºµn...")
        
        # Danh sÃ¡ch model Ä‘á»ƒ test
        test_models = ['llama3:8b', 'qwen2.5:14b', 'mixtral:latest', 'deepseek-coder:6.7b']
        self.available_models = []
        
        for model in test_models:
            try:
                # Test nhanh báº±ng API show
                response = requests.get(
                    f"{self.ollama_base}/api/show",
                    json={"name": model},
                    timeout=3
                )
                if response.status_code == 200:
                    self.available_models.append(model)
                    self.logger.info(f"  âœ… {model} cÃ³ sáºµn")
                else:
                    self.logger.debug(f"  âš ï¸ {model} khÃ´ng kháº£ dá»¥ng (HTTP {response.status_code})")
            except requests.exceptions.Timeout:
                self.logger.debug(f"  â° {model}: timeout")
            except Exception as e:
                self.logger.debug(f"  âŒ {model}: {str(e)[:50]}")
        
        # Náº¿u khÃ´ng tÃ¬m tháº¥y model, thá»­ kiá»ƒm tra káº¿t ná»‘i Ollama cÆ¡ báº£n
        if not self.available_models:
            try:
                response = requests.get(f"{self.ollama_base}/api/tags", timeout=5)
                if response.status_code == 200:
                    models_data = response.json().get('models', [])
                    self.available_models = [m['name'] for m in models_data]
                    self.logger.info(f"  ğŸ“Š TÃ¬m tháº¥y {len(self.available_models)} model tá»« API tags")
            except Exception as e:
                self.logger.warning(f"  KhÃ´ng thá»ƒ káº¿t ná»‘i Ä‘áº¿n Ollama: {e}")
    
    def _select_optimal_model(self):
        """Chá»n model tá»‘i Æ°u dá»±a trÃªn performance test"""
        # Æ¯u tiÃªn theo thá»© tá»± hiá»‡u nÄƒng vÃ  cháº¥t lÆ°á»£ng
        self.model_priority = {
            'chat': self._get_best_model_for_type('chat'),
            'coding': self._get_best_model_for_type('coding'),
            'web_search': self._get_best_model_for_type('research'),
            'research': self._get_best_model_for_type('research'),
            'reasoning': self._get_best_model_for_type('reasoning')
        }
        
        self.logger.info(f"ğŸ¯ Model tá»‘i Æ°u Ä‘Ã£ chá»n: {self.model_priority}")
    
    def _get_best_model_for_type(self, llm_type: str) -> str:
        """Chá»n model tá»‘t nháº¥t cho tá»«ng loáº¡i task"""
        # Æ¯u tiÃªn tá»« file cáº¥u hÃ¬nh
        if self.llm_profiles and 'profiles' in self.llm_profiles:
            profiles = self.llm_profiles['profiles']
            if llm_type in profiles and 'models' in profiles[llm_type]:
                for model in profiles[llm_type]['models']:
                    if model in self.available_models:
                        return model
        
        # Fallback priority
        priority_lists = {
            'chat': ['qwen2.5:14b', 'mixtral:latest', 'llama3:8b'],
            'coding': ['deepseek-coder:6.7b', 'codellama:7b', 'llama3:8b'],
            'research': ['qwen2.5:14b', 'mixtral:latest', 'llama3.1:latest', 'llama3:8b'],
            'reasoning': ['mixtral:latest', 'qwen2.5:14b', 'llama3.1:latest', 'llama3:8b']
        }
        
        priority = priority_lists.get(llm_type, ['llama3:8b'])
        for model in priority:
            if model in self.available_models:
                return model
        
        # Náº¿u khÃ´ng cÃ³ model nÃ o, tráº£ vá» model Ä‘áº§u tiÃªn cÃ³ sáºµn
        return self.available_models[0] if self.available_models else 'llama3:8b'
    
    def dispatch(self, plan: Dict[str, Any]) -> Dict[str, Any]:
        """
        Dispatch task Ä‘áº¿n model tá»‘i Æ°u nháº¥t
        
        Args:
            plan: Káº¿ hoáº¡ch tá»« ReasoningEngine
            
        Returns:
            Pháº£n há»“i tá»« LLM
        """
        if self.use_mock:
            self.logger.info("ğŸ“ Äang dÃ¹ng mock LLM")
            response = self._create_high_quality_mock_response(plan)
            return {
                "model": "mock",
                "response": response,
                "mode": "mock",
                "quality": "high"
            }
        
        llm_type = plan.get('llm_type', 'chat')
        model = self.model_priority.get(llm_type, 'llama3:8b')
        
        # Táº¡o prompt tá»‘i Æ°u
        prompt = self._create_optimized_prompt(plan, model)
        
        # Gá»i LLM vá»›i retry strategy thÃ´ng minh
        try:
            response = self._smart_llm_call(model, prompt, llm_type)
            
            # Náº¿u response khÃ´ng Ä‘á»§ tá»‘t, thá»­ model khÃ¡c
            if not self._is_response_adequate(response, llm_type):
                self.logger.warning(f"Response tá»« {model} khÃ´ng Ä‘á»§ tá»‘t, thá»­ model backup...")
                backup_model = self._get_backup_model(model, llm_type)
                if backup_model and backup_model != model:
                    response = self._smart_llm_call(backup_model, prompt, llm_type)
            
            return {
                "model": model,
                "response": response,
                "mode": "real",
                "quality": "high" if len(response) > 300 else "medium"
            }
            
        except Exception as e:
            self.logger.error(f"Lá»—i khi gá»i LLM: {e}")
            # Fallback sang mock response cháº¥t lÆ°á»£ng cao
            response = self._create_high_quality_mock_response(plan)
            return {
                "model": "mock-fallback",
                "response": response,
                "mode": "mock",
                "quality": "high"
            }
    
    def _create_optimized_prompt(self, plan: Dict[str, Any], model: str) -> str:
        """Táº¡o prompt tá»‘i Æ°u cho tá»«ng model"""
        intent = plan.get('intent', 'chat')
        user_input = plan.get('user_input', '')
        language = plan.get('language', 'vi')
        
        # Äáº¶C BIá»†T: Náº¿u lÃ  coding model (deepseek-coder), dÃ¹ng prompt tiáº¿ng Anh
        if 'coder' in model.lower() or 'code' in model.lower():
            return self._create_coding_specific_prompt(plan, model)
        
        # XÃ¡c Ä‘á»‹nh yÃªu cáº§u ngÃ´n ngá»¯ cho cÃ¡c model khÃ¡c
        lang_requirement = "Tráº£ lá»i báº±ng TIáº¾NG VIá»†T 100%." if language == 'vi' else "Answer in ENGLISH 100%."
        
        # Prompt base
        prompt_base = f"""{lang_requirement}

    YÃŠU Cáº¦U QUAN TRá»ŒNG:
    1. Tráº£ lá»i Äáº¦Y Äá»¦, CHI TIáº¾T, khÃ´ng cáº¯t ngang
    2. Tá»• chá»©c thÃ´ng tin cÃ³ cáº¥u trÃºc rÃµ rÃ ng
    3. ÄÆ°a vÃ­ dá»¥ cá»¥ thá»ƒ khi cÃ³ thá»ƒ

    CÃ‚U Há»I/ YÃŠU Cáº¦U: {user_input}

    Báº®T Äáº¦U TRáº¢ Lá»œI:"""
        
        # ThÃªm yÃªu cáº§u Ä‘áº·c biá»‡t theo intent
        if intent == 'coding':
            prompt_base += "\n\n[YÃŠU Cáº¦U CODE]\n- Code pháº£i Ä‘áº§y Ä‘á»§, cÃ³ thá»ƒ cháº¡y Ä‘Æ°á»£c\n- CÃ³ comment giáº£i thÃ­ch\n- CÃ³ vÃ­ dá»¥ sá»­ dá»¥ng\n- CÃ³ xá»­ lÃ½ lá»—i"
        elif intent in ['research', 'web_search']:
            prompt_base += "\n\n[YÃŠU Cáº¦U NGHIÃŠN Cá»¨U]\n- Cung cáº¥p thÃ´ng tin chi tiáº¿t, cÃ³ cáº¥u trÃºc\n- ÄÆ°a ra cÃ¡c khÃ­a cáº¡nh quan trá»ng\n- Káº¿t thÃºc vá»›i tÃ³m táº¯t"
        
        return prompt_base
    
    def _smart_llm_call(self, model: str, prompt: str, llm_type: str) -> str:
        """Gá»i LLM vá»›i strategy thÃ´ng minh"""
        # Äiá»u chá»‰nh timeout dá»±a trÃªn model
        timeouts = {
            'qwen2.5:14b': 45,
            'mixtral:latest': 60,
            'llama3:8b': 30,
            'deepseek-coder:6.7b': 40
        }
        timeout = timeouts.get(model, 30)
        
        # Äiá»u chá»‰nh max_tokens dá»±a trÃªn task
        max_tokens_map = {
            'coding': 4096,
            'research': 3072,
            'web_search': 3072,
            'chat': 2048
        }
        max_tokens = max_tokens_map.get(llm_type, 2048)
        
        try:
            payload = {
                "model": model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "num_predict": max_tokens,
                    "top_p": 0.9,
                    "repeat_penalty": 1.1
                }
            }
            
            self.logger.info(f"ğŸ¤– Gá»i {model} (timeout: {timeout}s, tokens: {max_tokens})")
            start_time = time.time()
            
            response = requests.post(
                f"{self.ollama_base}/api/generate",
                json=payload,
                timeout=timeout
            )
            
            elapsed = time.time() - start_time
            self.logger.info(f"â±ï¸  {model} pháº£n há»“i trong {elapsed:.2f}s")
            
            if response.status_code == 200:
                result = response.json().get('response', '').strip()
                
                # Kiá»ƒm tra vÃ  sá»­a response náº¿u cáº§n
                result = self._post_process_response(result, llm_type)
                
                return result
            else:
                raise Exception(f"API error: {response.status_code}")
                
        except requests.exceptions.Timeout:
            self.logger.error(f"â° Timeout vá»›i {model} sau {timeout}s")
            raise Exception(f"Model {model} timeout")
        except Exception as e:
            self.logger.error(f"âŒ Lá»—i vá»›i {model}: {e}")
            raise
    
    def _post_process_response(self, response: str, llm_type: str) -> str:
        """Xá»­ lÃ½ háº­u ká»³ Ä‘á»ƒ cáº£i thiá»‡n cháº¥t lÆ°á»£ng response"""
        if not response:
            return "Xin lá»—i, tÃ´i khÃ´ng thá»ƒ táº¡o pháº£n há»“i lÃºc nÃ y."
        
        # Äáº£m báº£o response khÃ´ng bá»‹ cáº¯t ngang
        endings = ['.', '!', '?', '```', '```python', '```bash']
        
        # Náº¿u response khÃ´ng káº¿t thÃºc báº±ng dáº¥u káº¿t thÃºc há»£p lÃ½
        last_char = response.strip()[-1] if response.strip() else ''
        
        if last_char not in endings and len(response) > 100:
            # TÃ¬m vá»‹ trÃ­ káº¿t thÃºc há»£p lÃ½ cuá»‘i cÃ¹ng
            for end_marker in ['.', '!', '?', '\n\n']:
                end_pos = response.rfind(end_marker)
                if end_pos > len(response) * 0.7:  # Náº¿u gáº§n cuá»‘i
                    response = response[:end_pos + 1]
                    break
        
        return response
    
    def _is_response_adequate(self, response: str, llm_type: str) -> bool:
        """Kiá»ƒm tra xem response cÃ³ Ä‘á»§ cháº¥t lÆ°á»£ng khÃ´ng"""
        if not response or len(response) < 50:
            return False
        
        # TiÃªu chÃ­ khÃ¡c nhau cho tá»«ng loáº¡i task
        if llm_type == 'coding':
            # Æ¯u tiÃªn deepseek-coder - kiá»ƒm tra khÃ¡c
            code_indicators = [
                'def ', 'class ', 'import ', 'from ', 'print(',
                'return ', 'for ', 'while ', 'if ', 'try:',
                '```python', '```bash', '```cpp', '```java'
            ]
            
            # deepseek-coder thÆ°á»ng cÃ³ response tá»‘t
            # Chá»‰ cáº§n cÃ³ má»™t trong cÃ¡c indicator lÃ  Ä‘á»§
            has_code = any(indicator in response for indicator in code_indicators)
            
            # deepseek-coder Ä‘áº·c biá»‡t tá»‘t, khÃ´ng cáº§n fallback trá»« khi ráº¥t ngáº¯n
            if len(response) > 100 and has_code:
                return True
            return False
            
        elif llm_type in ['research', 'web_search']:
            return len(response) > 200
        else:  # chat
            return len(response) > 100
    def _get_backup_model(self, primary_model: str, llm_type: str) -> str:
        """Láº¥y model backup náº¿u primary khÃ´ng tá»‘t"""
        backup_map = {
            'qwen2.5:14b': 'llama3:8b',
            'mixtral:latest': 'qwen2.5:14b',
            'llama3:8b': 'qwen2.5:14b',
            'deepseek-coder:6.7b': 'llama3:8b'
        }
        return backup_map.get(primary_model, 'llama3:8b')
    def _create_coding_specific_prompt(self, plan: Dict[str, Any], model: str) -> str:
        """
        Táº¡o prompt Ä‘áº·c biá»‡t cho coding models (tiáº¿ng Anh)
        """
        user_input = plan.get('user_input', '')
        
        # PhÃ¡t hiá»‡n ngÃ´n ngá»¯ láº­p trÃ¬nh tá»« input
        language_hints = {
            'python': ['python', 'pandas', 'numpy', 'def ', 'import '],
            'javascript': ['javascript', 'js', 'node', 'react', 'function('],
            'java': ['java', 'class ', 'public static'],
            'html': ['html', '<div>', '<p>', 'website'],
            'sql': ['sql', 'database', 'select ', 'insert ']
        }
        
        target_lang = 'python'  # Máº·c Ä‘á»‹nh
        for lang, hints in language_hints.items():
            if any(hint in user_input.lower() for hint in hints):
                target_lang = lang
                break
        
        # Prompt tiáº¿ng Anh cho deepseek-coder
        prompt = f"""You are an expert programming assistant. Write complete, runnable code in {target_lang.upper()}.

    USER REQUEST: {user_input}

    REQUIREMENTS:
    1. Write FULL, COMPLETE, RUNNABLE code
    2. Include comprehensive comments explaining the logic
    3. Include error handling
    4. Include example usage with test cases
    5. Use best practices and clean code principles

    RESPONSE FORMAT:
    - Start with a brief explanation of the solution
    - Then provide the complete code in a code block
    - End with example usage and expected output

    Complete {target_lang.upper()} code:"""
        
        return prompt
    def _create_high_quality_mock_response(self, plan: Dict[str, Any]) -> str:
        """Táº¡o mock response cháº¥t lÆ°á»£ng cao"""
        intent = plan.get('intent', 'chat')
        user_input = plan.get('user_input', '')
        language = plan.get('language', 'vi')
        
        if language == 'vi':
            return f"""**ORCHESTRATOR AI - CHáº¾ Äá»˜ DEMO CHáº¤T LÆ¯á»¢NG CAO**

**CÃ¢u há»i cá»§a báº¡n:** {user_input}

**ThÃ´ng tin há»‡ thá»‘ng:**
- Há»‡ thá»‘ng ORCHESTRATOR Ä‘ang cháº¡y á»Ÿ cháº¿ Ä‘á»™ DEMO
- Kiáº¿n trÃºc core AI báº¥t biáº¿n Ä‘Ã£ sáºµn sÃ ng
- Module chat tá»± nhiÃªn hoáº¡t Ä‘á»™ng á»•n Ä‘á»‹nh
- Há»‡ thá»‘ng permission Ä‘Æ°á»£c thiáº¿t láº­p Ä‘áº§y Ä‘á»§

**Äá»ƒ nháº­n pháº£n há»“i tá»« LLM thá»±c:**
1. Äáº£m báº£o Ollama Ä‘ang cháº¡y: `ollama serve`
2. Táº£i model phÃ¹ há»£p: `ollama pull qwen2.5:14b`
3. Khá»Ÿi Ä‘á»™ng láº¡i há»‡ thá»‘ng ORCHESTRATOR

**Gá»£i Ã½ cáº£i thiá»‡n:**
- Model qwen2.5:14b há»— trá»£ tiáº¿ng Viá»‡t tá»‘t nháº¥t
- Model llama3:8b nhanh vÃ  á»•n Ä‘á»‹nh
- Model deepseek-coder:6.7b chuyÃªn cho láº­p trÃ¬nh

**Tráº¡ng thÃ¡i hiá»‡n táº¡i:**
âœ… Kiáº¿n trÃºc há»‡ thá»‘ng hoÃ n chá»‰nh
âœ… Module xá»­ lÃ½ ngÃ´n ngá»¯ hoáº¡t Ä‘á»™ng
âœ… Permission vÃ  security ready
â³ Äang chá» káº¿t ná»‘i LLM thá»±c

HÃ£y káº¿t ná»‘i vá»›i Ollama Ä‘á»ƒ tráº£i nghiá»‡m Ä‘áº§y Ä‘á»§! ğŸš€"""
        else:
            return f"""**ORCHESTRATOR AI - HIGH QUALITY DEMO MODE**

**Your question:** {user_input}

**System information:**
- ORCHESTRATOR system is running in DEMO mode
- Immutable core AI architecture is ready
- Natural language chat module is operational
- Permission system is fully established

**To get real LLM responses:**
1. Ensure Ollama is running: `ollama serve`
2. Pull appropriate models: `ollama pull qwen2.5:14b`
3. Restart ORCHESTRATOR system

**Improvement suggestions:**
- qwen2.5:14b model has best Vietnamese support
- llama3:8b model is fast and stable
- deepseek-coder:6.7b model specializes in programming

**Current status:**
âœ… System architecture complete
âœ… Language processing module operational
âœ… Permission and security ready
â³ Waiting for real LLM connection

Connect to Ollama for full experience! ğŸš€"""