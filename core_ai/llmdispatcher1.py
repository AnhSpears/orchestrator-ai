"""
LLM DISPATCHER - ƒêi·ªÅu ph·ªëi LLM
Quy·∫øt ƒë·ªãnh g·ªçi LLM n√†o, v·ªõi prompt n√†o
"""
import yaml
import logging
import time
import requests
from typing import Dict, Any
import random

class LLMDispatcher:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.load_llm_profiles()
        self.ollama_base = "http://localhost:11434"
        self.use_mock = False
        self._check_ollama_connection()
        
    def _check_ollama_connection(self):
        """Ki·ªÉm tra k·∫øt n·ªëi Ollama khi kh·ªüi ƒë·ªông"""
        try:
            response = requests.get(f"{self.ollama_base}/api/tags", timeout=5)
            if response.status_code == 200:
                self.logger.info("‚úÖ K·∫øt n·ªëi ƒë·∫øn Ollama th√†nh c√¥ng!")
                self.use_mock = False
            else:
                self.logger.warning("‚ö†Ô∏è Ollama tr·∫£ v·ªÅ l·ªói, d√πng mock mode")
                self.use_mock = True
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn Ollama: {e}")
            self.logger.warning("üîß Chuy·ªÉn sang ch·∫ø ƒë·ªô mock (kh√¥ng c·∫ßn Ollama)")
            self.use_mock = True
    
    def load_llm_profiles(self):
        """T·∫£i c·∫•u h√¨nh LLM"""
        try:
            with open('config/llm_profiles.yaml', 'r') as f:
                self.llm_profiles = yaml.safe_load(f)
        except Exception as e:
            self.logger.error(f"Kh√¥ng th·ªÉ t·∫£i llm_profiles: {e}")
            self.llm_profiles = {}
    
    def dispatch(self, plan: Dict[str, Any]) -> Dict[str, Any]:
        """
        Dispatch task ƒë·∫øn LLM ph√π h·ª£p
        
        Args:
            plan: K·∫ø ho·∫°ch t·ª´ ReasoningEngine
            
        Returns:
            Ph·∫£n h·ªìi t·ª´ LLM
        """
        llm_type = plan.get('llm_type', 'chat')
        model = self._select_model(llm_type)
        
        # T·∫°o prompt ph√π h·ª£p
        prompt = self._create_detailed_prompt(plan)
        
        # G·ªçi LLM ho·∫∑c d√πng mock
        if self.use_mock:
            self.logger.info("üìù ƒêang d√πng mock LLM (kh√¥ng c·∫ßn k·∫øt n·ªëi m·∫°ng)")
            response = self._mock_llm_call(plan)
        else:
            try:
                response = self._call_llm_with_retry(model, prompt, llm_type)
            except Exception as e:
                self.logger.error(f"L·ªói khi g·ªçi LLM: {e}")
                response = self._mock_llm_call(plan)
        
        return {
            "model": model,
            "response": response,
            "prompt_length": len(prompt),
            "mode": "mock" if self.use_mock else "real"
        }
    
    def _select_model(self, llm_type: str) -> str:
        """Ch·ªçn model d·ª±a tr√™n lo·∫°i task"""
        profiles = self.llm_profiles.get('profiles', {})
        if llm_type in profiles:
            models = profiles[llm_type].get('models', [])
            if models:
                return models[0]  # L·∫•y model ƒë·∫ßu ti√™n
        
        # Fallback models
        fallbacks = {
            'chat': 'llama3:8b',
            'reasoning': 'mixtral:latest',
            'coding': 'deepseek-coder:6.7b'
        }
        return fallbacks.get(llm_type, 'llama3:8b')
    
    def _create_detailed_prompt(self, plan: Dict[str, Any]) -> str:
        """T·∫°o prompt chi ti·∫øt v√† c·∫•u tr√∫c cho task"""
        intent = plan.get('intent', 'chat')
        user_input = plan.get('user_input', '')
        language = plan.get('language', 'vi')
        
        # System prompts chi ti·∫øt cho t·ª´ng lo·∫°i task
        system_prompts = {
            'chat': {
                'vi': """B·∫°n l√† ORCHESTRATOR AI - tr·ª£ l√Ω th√¥ng minh, chuy√™n nghi·ªáp. 
H√£y tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß, chi ti·∫øt v√† h·ªØu √≠ch. 
∆Øu ti√™n cung c·∫•p th√¥ng tin c√≥ gi√° tr·ªã, kh√¥ng ch·ªâ tr·∫£ l·ªùi ng·∫Øn g·ªçn.
Lu√¥n gi·ªØ th√°i ƒë·ªô th√¢n thi·ªán, nhi·ªát t√¨nh.""",
                'en': """You are ORCHESTRATOR AI - an intelligent, professional assistant.
Provide complete, detailed, and helpful responses.
Prioritize giving valuable information, not just brief answers.
Always maintain a friendly and enthusiastic attitude."""
            },
            'web_search': {
                'vi': """B·∫°n l√† c√¥ng c·ª• t√¨m ki·∫øm th√¥ng minh. H√£y cung c·∫•p th√¥ng tin chi ti·∫øt, c√≥ c·∫•u tr√∫c:
1. T·ªïng quan v·ªÅ ch·ªß ƒë·ªÅ
2. C√°c ƒëi·ªÉm ch√≠nh quan tr·ªçng
3. ·ª®ng d·ª•ng th·ª±c t·∫ø
4. Xu h∆∞·ªõng hi·ªán t·∫°i
5. T√†i li·ªáu tham kh·∫£o (n·∫øu c√≥)

Th√¥ng tin ph·∫£i ch√≠nh x√°c, c√≥ t·ªï ch·ª©c v√† d·ªÖ hi·ªÉu.""",
                'en': """You are an intelligent search tool. Provide detailed, structured information:
1. Topic overview
2. Key important points
3. Practical applications
4. Current trends
5. References (if available)

Information must be accurate, organized, and easy to understand."""
            },
            'coding': {
                'vi': """B·∫°n l√† l·∫≠p tr√¨nh vi√™n chuy√™n nghi·ªáp. H√£y vi·∫øt code ƒë·∫ßy ƒë·ªß v·ªõi:
1. Code ho√†n ch·ªânh, c√≥ th·ªÉ ch·∫°y ƒë∆∞·ª£c
2. Comment gi·∫£i th√≠ch r√µ r√†ng
3. V√≠ d·ª• s·ª≠ d·ª•ng c·ª• th·ªÉ
4. Gi·∫£i th√≠ch logic v√† thu·∫≠t to√°n
5. X·ª≠ l√Ω c√°c tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát

Lu√¥n ∆∞u ti√™n code r√µ r√†ng, hi·ªáu qu·∫£ v√† d·ªÖ b·∫£o tr√¨.""",
                'en': """You are a professional programmer. Write complete code with:
1. Complete, runnable code
2. Clear explanatory comments
3. Specific usage examples
4. Explanation of logic and algorithms
5. Handling of special cases

Always prioritize clear, efficient, and maintainable code."""
            },
            'research': {
                'vi': """B·∫°n l√† nh√† nghi√™n c·ª©u chuy√™n nghi·ªáp. Cung c·∫•p nghi√™n c·ª©u chi ti·∫øt:
1. Gi·ªõi thi·ªáu v√† b·ªëi c·∫£nh
2. Ph∆∞∆°ng ph√°p nghi√™n c·ª©u
3. Ph√¢n t√≠ch chi ti·∫øt
4. K·∫øt qu·∫£ v√† ph√°t hi·ªán
5. K·∫øt lu·∫≠n v√† ƒë·ªÅ xu·∫•t
6. H∆∞·ªõng nghi√™n c·ª©u t∆∞∆°ng lai

Th√¥ng tin ph·∫£i s√¢u s·∫Øc, c√≥ cƒÉn c·ª© v√† h·ªØu √≠ch.""",
                'en': """You are a professional researcher. Provide detailed research:
1. Introduction and context
2. Research methodology
3. Detailed analysis
4. Results and findings
5. Conclusions and recommendations
6. Future research directions

Information must be insightful, evidence-based, and useful."""
            }
        }
        
        # L·∫•y prompt ph√π h·ª£p v·ªõi ng√¥n ng·ªØ
        prompt_template = system_prompts.get(intent, system_prompts['chat'])
        system = prompt_template.get(language, prompt_template['vi'])
        def _create_forceful_prompt(self, plan: Dict[str, Any]) -> str:
            """T·∫°o prompt v·ªõi y√™u c·∫ßu c·ª±c m·∫°nh ƒë·ªÉ ki·ªÉm so√°t response"""
            intent = plan.get('intent', 'chat')
            user_input = plan.get('user_input', '')
            language = plan.get('language', 'vi')
            
            # Y√äU C·∫¶U C·ª∞C M·∫†NH V·ªÄ NG√îN NG·ªÆ
            if language == 'vi':
                lang_force = """
    === QUY ƒê·ªäNH NG√îN NG·ªÆ B·∫ÆT BU·ªòC ===
    1. PH·∫¢I tr·∫£ l·ªùi 100% b·∫±ng TI·∫æNG VI·ªÜT
    2. KH√îNG ƒê∆Ø·ª¢C chuy·ªÉn sang ti·∫øng Anh b·∫•t k·ª≥ l√∫c n√†o
    3. KH√îNG ƒê∆Ø·ª¢C pha tr·ªôn ng√¥n ng·ªØ
    4. N·∫øu t·ª´ chuy√™n ng√†nh kh√¥ng c√≥ ti·∫øng Vi·ªát, gi·∫£i th√≠ch b·∫±ng ti·∫øng Vi·ªát
    """
            else:
                lang_force = """
    === MANDATORY LANGUAGE RULES ===
    1. MUST answer 100% in ENGLISH
    2. DO NOT switch to any other language
    3. DO NOT mix languages
    4. If technical terms don't exist in English, explain in English
    """
            
            # Y√äU C·∫¶U V·ªÄ ƒê·ªò D√ÄI
            length_requirements = """
    === Y√äU C·∫¶U ƒê·ªò D√ÄI B·∫ÆT BU·ªòC ===
    1. C√¢u tr·∫£ l·ªùi PH·∫¢I c√≥ √≠t nh·∫•t 500 t·ª´
    2. PH·∫¢I tri·ªÉn khai ƒë·∫ßy ƒë·ªß c√°c √Ω
    3. KH√îNG ƒê∆Ø·ª¢C c·∫Øt ng·∫Øn, b·ªè d·ªü
    4. N·∫øu ch∆∞a xong, ti·∫øp t·ª•c vi·∫øt cho ƒë·∫øn khi ho√†n ch·ªânh
    """
            
            # Prompt template cho t·ª´ng intent
            templates = {
                'chat': f"""
    ### VAI TR√í:
    B·∫°n l√† ORCHESTRATOR AI - tr·ª£ l√Ω th√¥ng minh, chuy√™n nghi·ªáp ng∆∞·ªùi Vi·ªát.

    ### QUY T·∫ÆC B·∫ÆT BU·ªòC:
    {lang_force}
    {length_requirements}

    ### Y√äU C·∫¶U C·ª§ TH·ªÇ:
    1. Tr·∫£ l·ªùi CHI TI·∫æT, ƒê·∫¶Y ƒê·ª¶
    2. T·ªï ch·ª©c th√¥ng tin c√≥ c·∫•u tr√∫c r√µ r√†ng
    3. ƒê∆∞a v√≠ d·ª• c·ª• th·ªÉ khi c√≥ th·ªÉ
    4. K·∫øt th√∫c v·ªõi ph·∫ßn t√≥m t·∫Øt

    ### C√ÇU H·ªéI: {user_input}

    ### TR·∫¢ L·ªúI (B·∫ÆT BU·ªòC D√ÄI, CHI TI·∫æT, B·∫∞NG TI·∫æNG VI·ªÜT):
    """,
                'coding': f"""
    ### VAI TR√í:
    B·∫°n l√† l·∫≠p tr√¨nh vi√™n chuy√™n nghi·ªáp.

    ### QUY T·∫ÆC B·∫ÆT BU·ªòC:
    {lang_force}
    {length_requirements}

    ### Y√äU C·∫¶U CODE:
    1. Code PH·∫¢I ƒë·∫ßy ƒë·ªß, c√≥ th·ªÉ ch·∫°y ƒë∆∞·ª£c
    2. C√≥ comment gi·∫£i th√≠ch t·ª´ng ph·∫ßn
    3. C√≥ v√≠ d·ª• s·ª≠ d·ª•ng c·ª• th·ªÉ
    4. C√≥ x·ª≠ l√Ω l·ªói ƒë·∫ßy ƒë·ªß
    5. C√≥ test cases

    ### Y√äU C·∫¶U: {user_input}

    ### CODE HO√ÄN CH·ªàNH (B·∫ÆT BU·ªòC ƒê·∫¶Y ƒê·ª¶):
    """,
                'web_search': f"""
    ### VAI TR√í:
    B·∫°n l√† c√¥ng c·ª• t√¨m ki·∫øm th√¥ng tin chuy√™n s√¢u.

    ### QUY T·∫ÆC B·∫ÆT BU·ªòC:
    {lang_force}
    {length_requirements}

    ### C·∫§U TR√öC TH√îNG TIN B·∫ÆT BU·ªòC:
    1. GI·ªöI THI·ªÜU: Kh√°i ni·ªám c∆° b·∫£n
    2. L·ªäCH S·ª¨: Qu√° tr√¨nh ph√°t tri·ªÉn
    3. ·ª®NG D·ª§NG: C√°c ·ª©ng d·ª•ng th·ª±c t·∫ø
    4. XU H∆Ø·ªöNG: Ph√°t tri·ªÉn hi·ªán t·∫°i
    5. TH√ÅCH TH·ª®C: V·∫•n ƒë·ªÅ c·∫ßn gi·∫£i quy·∫øt
    6. T√ÄI NGUY√äN: Ngu·ªìn tham kh·∫£o
    7. K·∫æT LU·∫¨N: T√≥m t·∫Øt v√† ƒë√°nh gi√°

    ### CH·ª¶ ƒê·ªÄ: {user_input}

    ### TH√îNG TIN CHI TI·∫æT (B·∫ÆT BU·ªòC ƒê·∫¶Y ƒê·ª¶):
    """
            }
            
            return templates.get(intent, templates['chat'])
        # T·∫°o prompt chi ti·∫øt d·ª±a tr√™n intent
        if intent == 'coding':
            return f"""### System Prompt (H·ªá th·ªëng l·∫≠p tr√¨nh)
{system}

### Y√™u c·∫ßu t·ª´ ng∆∞·ªùi d√πng:
{user_input}

### Y√™u c·∫ßu chi ti·∫øt:
1. **Ph√¢n t√≠ch y√™u c·∫ßu**: Hi·ªÉu r√µ v·∫•n ƒë·ªÅ c·∫ßn gi·∫£i quy·∫øt
2. **Thi·∫øt k·∫ø gi·∫£i ph√°p**: M√¥ t·∫£ c√°ch ti·∫øp c·∫≠n
3. **Tri·ªÉn khai code**: Vi·∫øt code ƒë·∫ßy ƒë·ªß
4. **Gi·∫£i th√≠ch code**: Comment v√† gi·∫£i th√≠ch logic
5. **V√≠ d·ª• s·ª≠ d·ª•ng**: Show how to use the code
6. **Ki·ªÉm th·ª≠**: C√°c test case quan tr·ªçng

### Code v√† gi·∫£i th√≠ch:"""
        
        elif intent == 'web_search':
            return f"""### System Prompt (C√¥ng c·ª• t√¨m ki·∫øm)
{system}

### Ch·ªß ƒë·ªÅ t√¨m ki·∫øm:
{user_input}

### Y√™u c·∫ßu t√¨m ki·∫øm chi ti·∫øt:
H√£y cung c·∫•p th√¥ng tin to√†n di·ªán v·ªÅ ch·ªß ƒë·ªÅ n√†y. Bao g·ªìm:

1. **T·ªïng quan**: Kh√°i ni·ªám c∆° b·∫£n, ƒë·ªãnh nghƒ©a
2. **L·ªãch s·ª≠ ph√°t tri·ªÉn**: Qu√° tr√¨nh h√¨nh th√†nh v√† ph√°t tri·ªÉn
3. **·ª®ng d·ª•ng th·ª±c t·∫ø**: C√°c ·ª©ng d·ª•ng trong ƒë·ªùi s·ªëng, c√¥ng nghi·ªáp
4. **Xu h∆∞·ªõng hi·ªán t·∫°i**: C√°c ph√°t tri·ªÉn m·ªõi nh·∫•t
5. **Th√°ch th·ª©c v√† c∆° h·ªôi**: Nh·ªØng v·∫•n ƒë·ªÅ v√† tri·ªÉn v·ªçng
6. **T√†i nguy√™n h·ªçc t·∫≠p**: C√°c ngu·ªìn tham kh·∫£o uy t√≠n

### Th√¥ng tin chi ti·∫øt:"""
        
        elif intent == 'research':
            return f"""### System Prompt (Nh√† nghi√™n c·ª©u)
{system}

### ƒê·ªÅ t√†i nghi√™n c·ª©u:
{user_input}

### Ph∆∞∆°ng ph√°p nghi√™n c·ª©u:
1. **X√°c ƒë·ªãnh v·∫•n ƒë·ªÅ**: L√†m r√µ ph·∫°m vi v√† m·ª•c ti√™u nghi√™n c·ª©u
2. **Thu th·∫≠p d·ªØ li·ªáu**: C√°c ngu·ªìn th√¥ng tin v√† ph∆∞∆°ng ph√°p thu th·∫≠p
3. **Ph√¢n t√≠ch d·ªØ li·ªáu**: C√°ch th·ª©c x·ª≠ l√Ω v√† ph√¢n t√≠ch th√¥ng tin
4. **T·ªïng h·ª£p k·∫øt qu·∫£**: C√°c ph√°t hi·ªán v√† k·∫øt lu·∫≠n
5. **ƒê·ªÅ xu·∫•t ·ª©ng d·ª•ng**: ·ª®ng d·ª•ng th·ª±c t·∫ø c·ªßa nghi√™n c·ª©u

### B√°o c√°o nghi√™n c·ª©u chi ti·∫øt:"""
        
        else:  # chat
            return f"""### System Prompt (Tr·ª£ l√Ω AI)
{system}

### Tin nh·∫Øn t·ª´ ng∆∞·ªùi d√πng:
{user_input}

### Y√™u c·∫ßu tr·∫£ l·ªùi:
H√£y tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß, chi ti·∫øt v√† h·ªØu √≠ch. C·∫•u tr√∫c ph·∫£n h·ªìi n√™n bao g·ªìm:

1. **L·ªùi ch√†o th√¢n thi·ªán**: M·ªü ƒë·∫ßu t√≠ch c·ª±c
2. **N·ªôi dung ch√≠nh**: Th√¥ng tin chi ti·∫øt, ƒë∆∞·ª£c t·ªï ch·ª©c r√µ r√†ng
3. **V√≠ d·ª• minh h·ªça**: C√°c v√≠ d·ª• c·ª• th·ªÉ n·∫øu c√≥
4. **L·ªùi khuy√™n h·ªØu √≠ch**: C√°c g·ª£i √Ω th·ª±c t·∫ø
5. **K·∫øt lu·∫≠n**: T√≥m t·∫Øt v√† ƒë·ªÅ xu·∫•t ti·∫øp theo

### Ph·∫£n h·ªìi c·ªßa b·∫°n:"""
    
    def _call_llm_with_retry(self, model: str, prompt: str, llm_type: str) -> str:
        """G·ªçi LLM v·ªõi retry v√† timeout linh ho·∫°t"""
        max_retries = 3
        timeout = 120 if llm_type == 'research' else 60
        
        for attempt in range(max_retries):
            try:
                self.logger.info(f"G·ªçi LLM l·∫ßn {attempt + 1}: {model}")
                
                payload = {
                    "model": model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "num_predict": 4096  # TƒÉng ƒë·ªô d√†i response
                    }
                }
                
                response = requests.post(
                    f"{self.ollama_base}/api/generate",
                    json=payload,
                    timeout=timeout
                )
                
                if response.status_code == 200:
                    result = response.json().get('response', '')
                    if len(result) < 100:  # N·∫øu response qu√° ng·∫Øn
                        self.logger.warning(f"Response qu√° ng·∫Øn ({len(result)} k√Ω t·ª±)")
                        if attempt < max_retries - 1:
                            time.sleep(2)
                            continue
                    return result
                else:
                    self.logger.warning(f"L·ªói API (l·∫ßn {attempt+1}): {response.status_code}")
                    if attempt < max_retries - 1:
                        time.sleep(2)
                        
            except requests.exceptions.Timeout:
                self.logger.warning(f"Timeout (l·∫ßn {attempt+1})")
                if attempt < max_retries - 1:
                    time.sleep(2)
                else:
                    raise Exception(f"Timeout sau {max_retries} l·∫ßn th·ª≠")
            except Exception as e:
                self.logger.warning(f"L·ªói (l·∫ßn {attempt+1}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(2)
                else:
                    raise
        
        raise Exception(f"Kh√¥ng th·ªÉ l·∫•y response sau {max_retries} l·∫ßn th·ª≠")
    def _call_llm_continued(self, model: str, prompt: str, max_tokens: int = 8192) -> str:
        """G·ªçi LLM v·ªõi kh·∫£ nƒÉng ti·∫øp t·ª•c n·∫øu response b·ªã c·∫Øt"""
        full_response = ""
        max_continuations = 3  # T·ªëi ƒëa 3 l·∫ßn ti·∫øp t·ª•c
        
        for continuation in range(max_continuations):
            try:
                self.logger.info(f"G·ªçi LLM (ti·∫øp t·ª•c {continuation + 1}): {model}")
                
                current_prompt = prompt if continuation == 0 else f"{full_response}\n\n[TI·∫æP T·ª§C: H√£y vi·∫øt th√™m, ch∆∞a ƒë∆∞·ª£c c·∫Øt ngang]"
                
                payload = {
                    "model": model,
                    "prompt": current_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "num_predict": max_tokens,
                        "top_p": 0.9,
                        "repeat_penalty": 1.1,
                        "stop": ["###", "```end", "[END]"]  # Stop tokens
                    }
                }
                
                response = requests.post(
                    f"{self.ollama_base}/api/generate",
                    json=payload,
                    timeout=180  # 3 ph√∫t timeout
                )
                
                if response.status_code == 200:
                    result = response.json().get('response', '')
                    full_response += result
                    
                    # Ki·ªÉm tra n·∫øu response ƒë√£ ho√†n ch·ªânh
                    if self._is_complete_response(result):
                        self.logger.info(f"Response ho√†n ch·ªânh sau {continuation + 1} l·∫ßn")
                        break
                    else:
                        self.logger.warning(f"Response c√≥ th·ªÉ b·ªã c·∫Øt, ti·∫øp t·ª•c...")
                        time.sleep(1)
                else:
                    self.logger.error(f"L·ªói API: {response.status_code}")
                    break
                    
            except Exception as e:
                self.logger.error(f"L·ªói khi ti·∫øp t·ª•c: {e}")
                break
        
        return full_response
    
    def _is_complete_response(self, response: str) -> bool:
        """Ki·ªÉm tra xem response ƒë√£ ho√†n ch·ªânh ch∆∞a"""
        # N·∫øu response qu√° ng·∫Øn
        if len(response.strip()) < 100:
            return False
        
        # N·∫øu k·∫øt th√∫c b·∫±ng d·∫•u c√¢u ho√†n ch·ªânh
        endings = ['.', '!', '?', '```', '###']
        last_char = response.strip()[-1] if response.strip() else ''
        
        # N·∫øu c√≥ t·ª´ ch·ªâ s·ª± ho√†n th√†nh
        completion_indicators = [
            'k·∫øt th√∫c', 't·∫°m k·∫øt', 'tr√™n ƒë√¢y', 't√≥m l·∫°i',
            'end', 'conclusion', 'summary', 'in conclusion'
        ]
        
        last_50_chars = response.strip()[-50:].lower()
        has_completion_indicator = any(indicator in last_50_chars for indicator in completion_indicators)
        
        return last_char in endings or has_completion_indicator
    def _select_best_model(self, llm_type: str) -> str:
        """Ch·ªçn model t·ªët nh·∫•t cho task"""
        # ∆Øu ti√™n c√°c model m·∫°nh, nhi·ªÅu token
        model_priority = {
            'chat': ['mixtral:latest', 'qwen2.5:14b', 'llama3:8b'],
            'coding': ['deepseek-coder:6.7b', 'codellama:7b', 'llama3:8b'],
            'web_search': ['mixtral:latest', 'qwen2.5:14b', 'llama3.1:latest'],
            'research': ['qwen2.5:14b', 'mixtral:latest', 'llama3.1:latest']
        }
        
        available_models = self._get_available_models()
        priority_list = model_priority.get(llm_type, ['mixtral:latest', 'llama3:8b'])
        
        # Ch·ªçn model c√≥ s·∫µn ƒë·∫ßu ti√™n trong danh s√°ch ∆∞u ti√™n
        for model in priority_list:
            if model in available_models:
                return model
        
        # Fallback
        return 'llama3:8b'
    
    def _get_available_models(self):
        """L·∫•y danh s√°ch model c√≥ s·∫µn t·ª´ Ollama"""
        try:
            response = requests.get(f"{self.ollama_base}/api/tags", timeout=5)
            if response.status_code == 200:
                models = [model['name'] for model in response.json().get('models', [])]
                return models
        except:
            pass
        return ['llama3:8b']  # Fallback
    def _mock_llm_call(self, plan: Dict[str, Any]) -> str:
        """Mock LLM call chi ti·∫øt h∆°n"""
        intent = plan.get('intent', 'chat')
        user_input = plan.get('user_input', '')
        language = plan.get('language', 'vi')
        
        # Mock responses chi ti·∫øt h∆°n
        mock_responses = {
            'coding': {
                'vi': f"""**PH√ÇN T√çCH Y√äU C·∫¶U**: {user_input}

**GI·∫¢I PH√ÅP**:
1. **Ph√¢n t√≠ch v·∫•n ƒë·ªÅ**: Hi·ªÉu r√µ y√™u c·∫ßu v√† x√°c ƒë·ªãnh c√°c tr∆∞·ªùng h·ª£p c·∫ßn x·ª≠ l√Ω
2. **Thi·∫øt k·∫ø thu·∫≠t to√°n**: L·ª±a ch·ªçn thu·∫≠t to√°n ph√π h·ª£p, t·ªëi ∆∞u hi·ªáu su·∫•t
3. **Tri·ªÉn khai code**: Vi·∫øt code r√µ r√†ng, c√≥ c·∫•u tr√∫c t·ªët
4. **X·ª≠ l√Ω l·ªói**: D·ª± ƒëo√°n v√† x·ª≠ l√Ω c√°c l·ªói c√≥ th·ªÉ x·∫£y ra

**CODE M·∫™U**:
"""},}


def main():
    print("Ch·ª©c nƒÉng ƒëang ƒë∆∞·ª£c ph√°t tri·ªÉn")
    print("K·∫øt n·ªëi ƒë·∫øn LLM ƒë·ªÉ nh·∫≠n code ƒë·∫ßy ƒë·ªß")
    
if __name__ == "__main__":
    main()