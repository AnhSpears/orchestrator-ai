"""
Script t·ªëi ∆∞u h√≥a Ollama cho h·ªá th·ªëng ORCHESTRATOR
"""
import subprocess
import sys
import os

def check_ollama():
    """Ki·ªÉm tra Ollama ƒë√£ c√†i ƒë·∫∑t ch∆∞a"""
    try:
        result = subprocess.run(['ollama', '--version'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            print("‚úÖ Ollama ƒë√£ c√†i ƒë·∫∑t")
            return True
        else:
            print("‚ùå Ollama kh√¥ng kh·∫£ d·ª•ng")
            return False
    except FileNotFoundError:
        print("‚ùå Ollama ch∆∞a c√†i ƒë·∫∑t")
        return False

def check_models():
    """Ki·ªÉm tra model ƒë√£ c√≥"""
    try:
        result = subprocess.run(['ollama', 'list'], 
                              capture_output=True, text=True)
        print("üìã Model hi·ªán c√≥:")
        print(result.stdout)
        return True
    except:
        print("‚ö†Ô∏è Kh√¥ng th·ªÉ ki·ªÉm tra model")
        return False

def recommend_models():
    """ƒê·ªÅ xu·∫•t model t·ªëi ∆∞u"""
    print("\nüéØ ƒê·ªÄ XU·∫§T MODEL T·ªêI ∆ØU:")
    print("="*50)
    print("1. qwen2.5:14b - T·ªët nh·∫•t cho ti·∫øng Vi·ªát")
    print("   - H·ªó tr·ª£ ti·∫øng Vi·ªát xu·∫•t s·∫Øc")
    print("   - Context 32K tokens")
    print("   - Dung l∆∞·ª£ng: ~8GB")
    print("")
    print("2. llama3:8b - Nhanh, ·ªïn ƒë·ªãnh")
    print("   - T·ªëc ƒë·ªô nhanh")
    print("   - Ti·∫øng Anh t·ªët, ti·∫øng Vi·ªát kh√°")
    print("   - Dung l∆∞·ª£ng: ~4.7GB")
    print("")
    print("3. deepseek-coder:6.7b - Chuy√™n code")
    print("   - Code xu·∫•t s·∫Øc")
    print("   - H·ªó tr·ª£ nhi·ªÅu ng√¥n ng·ªØ l·∫≠p tr√¨nh")
    print("   - Dung l∆∞·ª£ng: ~4GB")
    print("")
    print("üì• L·ªánh t·∫£i model:")
    print("  ollama pull qwen2.5:14b")
    print("  ollama pull llama3:8b")
    print("  ollama pull deepseek-coder:6.7b")

def optimize_settings():
    """T·ªëi ∆∞u c√†i ƒë·∫∑t Ollama"""
    print("\n‚öôÔ∏è C√ÄI ƒê·∫∂T T·ªêI ∆ØU:")
    print("="*50)
    
    settings = """
# Th√™m v√†o bi·∫øn m√¥i tr∆∞·ªùng ho·∫∑c ch·∫°y tr∆∞·ªõc khi start ollama

# Windows (PowerShell):
$env:OLLAMA_NUM_GPU = 1
$env:OLLAMA_MAX_VRAM = "6144"  # 6GB VRAM

# Linux/Mac:
export OLLAMA_NUM_GPU=1
export OLLAMA_MAX_VRAM="6144"

# Kh·ªüi ƒë·ªông Ollama v·ªõi nhi·ªÅu resource:
ollama serve --num-gpu 1
"""
    print(settings)
    
    # T·∫°o file batch cho Windows
    if sys.platform == "win32":
        with open("start_ollama_optimized.bat", "w") as f:
            f.write("""@echo off
set OLLAMA_NUM_GPU=1
set OLLAMA_MAX_VRAM=6144
echo Starting Ollama with optimized settings...
ollama serve
pause""")
        print("‚úÖ ƒê√£ t·∫°o file start_ollama_optimized.bat")

def create_model_profiles():
    """T·∫°o profile model t·ªëi ∆∞u"""
    profiles = {
        "chat": {
            "primary": "qwen2.5:14b",
            "fallback": "llama3:8b",
            "timeout": 45,
            "max_tokens": 4096
        },
        "coding": {
            "primary": "deepseek-coder:6.7b", 
            "fallback": "llama3:8b",
            "timeout": 60,
            "max_tokens": 8192
        },
        "research": {
            "primary": "qwen2.5:14b",
            "fallback": "llama3:8b",
            "timeout": 60,
            "max_tokens": 8192
        }
    }
    
    with open("optimal_model_profiles.yaml", "w") as f:
        import yaml
        yaml.dump(profiles, f, default_flow_style=False, allow_unicode=True)
    
    print("‚úÖ ƒê√£ t·∫°o file optimal_model_profiles.yaml")

def main():
    """H√†m ch√≠nh"""
    print("="*60)
    print("T·ªêI ∆ØU H√ìA OLLAMA CHO ORCHESTRATOR")
    print("="*60)
    
    if not check_ollama():
        print("\n‚ö†Ô∏è Vui l√≤ng c√†i ƒë·∫∑t Ollama tr∆∞·ªõc:")
        print("  https://ollama.com/download")
        return
    
    check_models()
    recommend_models()
    optimize_settings()
    create_model_profiles()
    
    print("\n" + "="*60)
    print("‚úÖ HO√ÄN T·∫§T T·ªêI ∆ØU H√ìA")
    print("="*60)
    print("\nüìã H√ÄNH ƒê·ªòNG TI·∫æP THEO:")
    print("1. T·∫£i model ƒë·ªÅ xu·∫•t: ollama pull qwen2.5:14b")
    print("2. Kh·ªüi ƒë·ªông Ollama: start_ollama_optimized.bat")
    print("3. Ch·∫°y ORCHESTRATOR: python main.py")
    print("4. Ki·ªÉm tra: python test_model_capabilities.py")

if __name__ == "__main__":
    main()